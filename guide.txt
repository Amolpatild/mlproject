# ML project

1. open the anaconda propmt---> change the dir
2. code .(open VS code)--> Terminal-->command prompt
3. create conda enviornment-->conda create -p venv python==3.8 -y
4. conda activate venv/
5. Create README.md
-->git add README.md
-->git commit -m "First Commit"
-->git branch -M main
-->git remote add origin https://github.com/Amolpatild/mlproject.git
-->git config --global user.name "John Doe"
-->git config --global user.email johndoe@example.com
-->git remote -v-->git push -u origin main
6. Create gitignore file in github-->add file-->create new file-->.gitignore-->type Python under the template and select it-->commit change .gitignore-->git pull

7. create setup.py and requirements.txt(write pandas,numpy, seaborn, -e .)
requirements.txt-->
pandas
numpy
seaborn
matplotlib
scikit-learn
catboost
xgboost
-e .# this is used to initialize the setup.py

8. in setup.py write code below:

from setuptools import find_packages,setup
from typing import List

HYPEN_E_DOT = '-e .'
def get_requirements(file_path:str)->List[str]:
    '''
    This fucntion returns list of enviornments
    '''

    requirements = []
    with open(file_path) as file_obj:
        requirements = file_obj.readlines()
        requirements = [req.replace('\n',' ') for req in requirements]
        if HYPEN_E_DOT in requirements:
            requirements.remove(HYPEN_E_DOT)
    return requirements

setup(

    name = 'mlproject',
    version='0.0.1',
    author='amol',
    author_email='patilamol011@gmail.com',
    packages=find_packages(),
    install_requires = get_requirements('requirements.txt'),

)


9. create folder src-->create file __init__.py-->this src folder will be the package
10. pip install -r requirements.txt
-->git add .
-->git status
-->git commit -m "The second commit"
-->git push -u origin main

11. create the component folder under the src folder --why-->it's require for the data ingenstion process.
-->create __init__.py file
-->create data_ingestion.py file
-->create data_trasnformation.py file
-->create model_trainer.py file

12. create pipeline folder
-->create __init__.py file
-->create prediction_pipeline.py file
-->create training_pipeline.py file

13. create the files under src folder
-->create utils.py file
-->create logger.py file
-->create exception.py file

14. write exception

import sys
from src.logger import  logging

def error_message_detail(error,error_detail:sys):
    _,_,exc_tb = error_detail.exc_info()
    file_name = exc_tb.tb_frame.f_code.co_filename
    error_message = "Error occured in python script name[{0}] line number[{1}] error message[{2}]".format(
        file_name,exc_tb.tb_lineno,str(error)
    )
    return error_message

class CustomException(Exception):
    def __init__(self, error_message,error_detail:sys):
        super().__init__(error_message)
        self.error_message = error_message_detail(error_message,error_detail=error_detail)

    def __str__(self):
        return self.error_message

**to test the file working or not******
                                            
if __name__ == "__main__":

    try:
        a = 1/0
    except Exception as e:
        logging.info("Logging has started")
        raise CustomException(e,sys)
****************************************

15. write logger
import logging
import os
from datetime import datetime

LOG_FILE = f"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log"
logs_path = os.path.join(os.getcwd(),'logs',LOG_FILE)
os.makedirs(logs_path,exist_ok=True)

LOG_FILE_PATH = os.path.join(logs_path,LOG_FILE)

logging.basicConfig(
    filename= LOG_FILE_PATH,
    format="[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)

****to test the file working or not*****
if __name__ == "__main__":
    logging.info("Logging has started")
****************************************

remove the test part from logger and exeption files and push on git hub


16. create notebook folder
-->create data folder and add csv file and notebook files
-->run the notebook files 

17. write in data ingestion.py(components)

import os
import sys
from src.exception import CustomException
from src.logger import logging
import pandas as pd
from sklearn.model_selection import train_test_split
from dataclasses import dataclass

@dataclass
class DataIngestionConfig:
    train_data_path:str=os.path.join('artifacts','train.csv')
    test_data_path:str=os.path.join('artifacts','test.csv')
    raw_data_path:str=os.path.join('artifacts','data.csv')

class DataIngestion:
    def __init__(self):
        self.ingestion_config = DataIngestionConfig()

    def initiate_data_ingestion(self):
        logging.info("Enter the data ingestion method or component")
        try:
            df = pd.read_csv('notebook\data\stud.csv')
            logging.info("Read the data successfully")
            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path),exist_ok=True)

            df.to_csv(self.ingestion_config.raw_data_path, index=False,header=True)
            logging.info("Train test split initiated")
            train_set, test_set = train_test_split(df,test_size=0.2,random_state= 42)

            train_set.to_csv(self.ingestion_config.train_data_path,index=False,header=True)
            test_set.to_csv(self.ingestion_config.test_data_path,index=False,header=True)

            logging.info("Ingestion of data compleated")

            return(
                self.ingestion_config.train_data_path,
                self.ingestion_config.test_data_path,
                

            )
        except Exception as e:
            raise CustomException(e,sys)
        

if __name__ == "__main__":
    obj = DataIngestion()
    obj.initiate_data_ingestion()




open the terminal --> python src/components/data_ingestion.py-->check artifacts are created and log file as well.



                                            