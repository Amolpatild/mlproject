# ML project

1. open the anaconda propmt---> change the dir
2. code .(open VS code)--> Terminal-->command prompt
3. create conda enviornment-->conda create -p venv python==3.8 -y
4. conda activate venv/
5. Create README.md
-->git add README.md
-->git commit -m "First Commit"
-->git branch -M main
-->git remote add origin https://github.com/Amolpatild/mlproject.git
-->git config --global user.name "John Doe"
-->git config --global user.email johndoe@example.com
-->git remote -v-->git push -u origin main
6. Create gitignore file in github-->add file-->create new file-->.gitignore-->type Python under the template and select it-->commit change .gitignore-->git pull

7. create setup.py and requirements.txt(write pandas,numpy, seaborn, -e .)
requirements.txt-->
pandas
numpy
seaborn
matplotlib
scikit-learn
catboost
xgboost
-e .# this is used to initialize the setup.py

8. in setup.py write code below:

from setuptools import find_packages,setup
from typing import List

HYPEN_E_DOT = '-e .'
def get_requirements(file_path:str)->List[str]:
    '''
    This fucntion returns list of enviornments
    '''

    requirements = []
    with open(file_path) as file_obj:
        requirements = file_obj.readlines()
        requirements = [req.replace('\n',' ') for req in requirements]
        if HYPEN_E_DOT in requirements:
            requirements.remove(HYPEN_E_DOT)
    return requirements

setup(

    name = 'mlproject',
    version='0.0.1',
    author='amol',
    author_email='patilamol011@gmail.com',
    packages=find_packages(),
    install_requires = get_requirements('requirements.txt'),

)


9. create folder src-->create file __init__.py-->this src folder will be the package
10. pip install -r requirements.txt
-->git add .
-->git status
-->git commit -m "The second commit"
-->git push -u origin main

11. create the component folder under the src folder --why-->it's require for the data ingenstion process.
-->create __init__.py file
-->create data_ingestion.py file
-->create data_trasnformation.py file
-->create model_trainer.py file

12. create pipeline folder
-->create __init__.py file
-->create prediction_pipeline.py file
-->create training_pipeline.py file

13. create the files under src folder
-->create utils.py file
-->create logger.py file
-->create exception.py file

14. write exception

import sys
from src.logger import  logging

def error_message_detail(error,error_detail:sys):
    _,_,exc_tb = error_detail.exc_info()
    file_name = exc_tb.tb_frame.f_code.co_filename
    error_message = "Error occured in python script name[{0}] line number[{1}] error message[{2}]".format(
        file_name,exc_tb.tb_lineno,str(error)
    )
    return error_message

class CustomException(Exception):
    def __init__(self, error_message,error_detail:sys):
        super().__init__(error_message)
        self.error_message = error_message_detail(error_message,error_detail=error_detail)

    def __str__(self):
        return self.error_message

**to test the file working or not******
                                            
if __name__ == "__main__":

    try:
        a = 1/0
    except Exception as e:
        logging.info("Logging has started")
        raise CustomException(e,sys)
****************************************

15. write logger
import logging
import os
from datetime import datetime

LOG_FILE = f"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log"
logs_path = os.path.join(os.getcwd(),'logs',LOG_FILE)
os.makedirs(logs_path,exist_ok=True)

LOG_FILE_PATH = os.path.join(logs_path,LOG_FILE)

logging.basicConfig(
    filename= LOG_FILE_PATH,
    format="[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)

****to test the file working or not*****
if __name__ == "__main__":
    logging.info("Logging has started")
****************************************

remove the test part from logger and exeption files and push on git hub


16. create notebook folder
-->create data folder and add csv file and notebook files
-->run the notebook files 

17. write in data ingestion.py(components)

import os
import sys
from src.exception import CustomException
from src.logger import logging
import pandas as pd
from sklearn.model_selection import train_test_split
from dataclasses import dataclass

@dataclass
class DataIngestionConfig:
    train_data_path:str=os.path.join('artifacts','train.csv')
    test_data_path:str=os.path.join('artifacts','test.csv')
    raw_data_path:str=os.path.join('artifacts','data.csv')

class DataIngestion:
    def __init__(self):
        self.ingestion_config = DataIngestionConfig()

    def initiate_data_ingestion(self):
        logging.info("Enter the data ingestion method or component")
        try:
            df = pd.read_csv('notebook\data\stud.csv')
            logging.info("Read the data successfully")
            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path),exist_ok=True)

            df.to_csv(self.ingestion_config.raw_data_path, index=False,header=True)
            logging.info("Train test split initiated")
            train_set, test_set = train_test_split(df,test_size=0.2,random_state= 42)

            train_set.to_csv(self.ingestion_config.train_data_path,index=False,header=True)
            test_set.to_csv(self.ingestion_config.test_data_path,index=False,header=True)

            logging.info("Ingestion of data compleated")

            return(
                self.ingestion_config.train_data_path,
                self.ingestion_config.test_data_path,
                

            )
        except Exception as e:
            raise CustomException(e,sys)
        

if __name__ == "__main__":
    obj = DataIngestion()
    obj.initiate_data_ingestion()




open the terminal --> python src/components/data_ingestion.py-->check artifacts are created and log file as well.

add .artificats line under the gitignore file in envrionment


18. data transformation

import sys
import os
from dataclasses import dataclass
from src.utils import save_object

import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

from src.exception import CustomException
from src.logger import logging

@dataclass
class DataTransformationConfig:
    preprocessor_obj_file_path = os.path.join('artifacts','preprocessor.pkl')


class DataTransformation:
    def __init__(self):
        self.data_transformation_config = DataTransformationConfig()


    def get_data_transformer_object(self):

        '''
        This fucntion is responsible for data transformation
        '''
        try:
            numerical_colums = ['reading_score', 'writing_score']
            categorical_colums = [
                                'gender',
                                'race_ethnicity',
                                'parental_level_of_education',
                                'lunch',
                                'test_preparation_course'
                                ]
            numerical_pipeline = Pipeline(
                                steps=[
                                    ("imputer",SimpleImputer(strategy='median')),
                                    ('scaler',StandardScaler())
                                ]
            )
            categorical_pipline = Pipeline(
                                steps=[
                                    ('imputer',SimpleImputer(strategy='most_frequent')),
                                    ('one_hot_encoder',OneHotEncoder()),
                                    ('scaler',StandardScaler(with_mean=False))
                                ]
            )
            logging.info("Numberical columns standad scaling compleated")

            logging.info("Categorical columns endcoding compleated")

            preprocessor = ColumnTransformer(
                [
                ('num_pipeline',numerical_pipeline,numerical_colums),
                ('categorical_pipeline',categorical_pipline,categorical_colums)
                ]
            )
            
            return preprocessor
        
        except Exception as e:
            raise CustomException(e,sys)
        

    def initiate_data_transformation(self,train_path,test_path):
        try:
            train_df = pd.read_csv(train_path)
            test_df = pd.read_csv(test_path)

            logging.info('Read train and test data compleated')

            logging.info('Obtaining preprocessing object')

            preprocessing_object = self.get_data_transformer_object()

            target_column_name = 'math_score'
            numerical_colums = ['reading_score', 'writing_score']

            input_feature_train_df = train_df.drop(columns=[target_column_name],axis=1)
            target_feature_train_df = train_df[target_column_name]

            input_feature_test_df = test_df.drop(columns=[target_column_name],axis=1)
            target_feature_test_df = test_df[target_column_name]
            
            

            logging.info('Applying preprocessiong on the train and test data')

            input_feature_train_arr =preprocessing_object.fit_transform(input_feature_train_df)
            input_feature_test_arr =preprocessing_object.transform(input_feature_test_df)
            
            


            train_arr = np.c_[
                input_feature_train_arr,np.array(target_feature_train_df)
            ]
            test_arr = np.c_[input_feature_test_arr,np.array(target_feature_test_df)]

            logging.info('Saved the preprocessing object.')

            save_object(
                file_path = self.data_transformation_config.preprocessor_obj_file_path,
                obj = preprocessing_object
            )

            return (
                train_arr,
                test_arr,
                self.data_transformation_config.preprocessor_obj_file_path,
            )
        except Exception as e:
            raise CustomException(e,sys)

19. Write code in utils.py 

import os
import sys
from src.exception import CustomException
import dill

import numpy as np
import pandas as pd

def save_object(file_path,obj):
    try:
        dir_path = os.path.dirname(file_path)

        os.makedirs(dir_path,exist_ok=True)

        with open(file_path,'wb') as file_obj:
            dill.dump(obj,file_obj)

    except Exception as e:
        raise CustomException(e,sys)

====================================================

Add code at the end of data_ingestion.py

if __name__ == "__main__":
    obj = DataIngestion()
    train_data,test_data = obj.initiate_data_ingestion()
    data_transformation = DataTransformation()
    data_transformation.initiate_data_transformation(train_data,test_data)


21. add code in utils.py

import os
import sys
from src.exception import CustomException
import dill
import pickle
from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV

import numpy as np
import pandas as pd


def evaluate_models(X_train, y_train,X_test,y_test,models,param):
    try:
        report = {}

        for i in range(len(list(models))):
            model = list(models.values())[i]
            para=param[list(models.keys())[i]]

            gs = GridSearchCV(model,para,cv=3)
            gs.fit(X_train,y_train)

            model.set_params(**gs.best_params_)
            model.fit(X_train,y_train)

            #model.fit(X_train, y_train)  # Train model

            y_train_pred = model.predict(X_train)

            y_test_pred = model.predict(X_test)

            train_model_score = r2_score(y_train, y_train_pred)

            test_model_score = r2_score(y_test, y_test_pred)

            report[list(models.keys())[i]] = test_model_score

        return report

    except Exception as e:
        raise CustomException(e, sys)




                                            